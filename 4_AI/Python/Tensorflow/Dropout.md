### Слой регуляризации для предотвращения переобучения

#### Что это?

Dropout — это слой, который случайным образом "выключает" (обнуляет) определённый процент нейронов во время обучения. Это помогает предотвратить переобучение (overfitting), когда модель слишком хорошо запоминает тренировочные данные и плохо обобщает на новые.

#### Как работает?

- **Во время обучения**:
    - Задаётся доля нейронов для "выключения" (например, 0.2 = 20%).
    - Случайно выбранные нейроны обнуляются на каждом шаге обучения.
    - Это заставляет сеть учиться более устойчивым и независимым признакам.
- **Во время тестирования/предсказания**:
    - Все нейроны активны, но их выходы масштабируются (умножаются на 1 - доля выключения), чтобы сохранить ожидаемую сумму активаций.

#### Пример (хотя в коде не используется):

python

CollapseWrapCopy

`keras.layers.Dropout(0.2)`

- 20% нейронов будут случайным образом обнулены на каждом шаге обучения.

#### Почему не используется в коде?

- В данном примере модель простая (два Dense слоя), и переобучение не является серьёзной проблемой на Fashion MNIST с 10 эпохами.
- Dropout обычно добавляют в более глубокие сети или при большом числе эпох, когда точность на тренировочных данных сильно превышает тестовую.

#### Когда использовать?

- При глубокой архитектуре (много слоёв).
- Когда модель переобучается (тренировочная точность >> тестовой).

#### Интересный факт:

Dropout был предложен в 2014 году (Hinton et al.) и вдохновлён идеей, что случайное отключение нейронов делает сеть похожей на ансамбль множества подмоделей.