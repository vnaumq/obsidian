[[Data Management Architecture]]

Data lakehouses seek to resolve the core challenges across both data warehouses and data lakes to yield a more ideal data management solution for organizations. They represent the next evolution of data management solutions in the market.

A data lakehouse is a data platform, which merges the best aspects of data warehouses and [data lakes](https://www.ibm.com/topics/data-lake) into one [data management solution](https://www.ibm.com/data-management). [Data warehouses](https://www.ibm.com/topics/data-warehouse) tend to be more performant than data lakes, but they can be more expensive and limited in their ability to scale. A data lakehouse attempts to solve for this by leveraging cloud object storage to store a broader range of data types—that is, structured data, unstructured data and semi-structured data. By bringing these benefits under one [data architecture](https://www.ibm.com/topics/data-architecture), data teams can accelerate their data processing as they no longer need to straddle two disparate data systems to complete and scale more advanced analytics, such as [machine learning](https://www.ibm.com/think/topics/machine-learning).

## Key features of a data lakehouse

As previously noted, data lakehouses combine the best features within data warehousing with the most optimal ones within data lakes. It leverages similar data structures from data warehouses and pairs it with the low cost storage and flexibility of data lakes, enabling organizations to store and access big data quickly and more efficiently while also allowing them to mitigate potential data quality issues. It supports diverse data datasets, i.e. both structured and unstructured data, meeting the needs of both business intelligence and data science workstreams. It typically supports programming languages like Python, R, and high performance SQL.

Data lakehouses also support ACID transactions on larger data workloads. ACID stands for atomicity, consistency, isolation, and durability; all of which are key properties that define a transaction to ensure data integrity. Atomicity can be defined as all changes to data are performed as if they are a single operation. Consistency is when data is in a consistent state when a transaction starts and when it ends. Isolation refers to the intermediate state of transaction being invisible to other transactions. As a result, transactions that run concurrently appear to be serialized. Durability is after a transaction successfully completes, changes to data persist and are not undone, even in the event of a system failure. This feature is critical in ensuring data consistency as multiple users read and write data simultaneously.